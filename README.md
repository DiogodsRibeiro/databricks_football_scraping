# ‚öΩ Football Analytics Pipeline - Databricks & Azure

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![Databricks](https://img.shields.io/badge/Databricks-Delta_Live_Tables-red.svg)](https://www.databricks.com/)
[![Azure](https://img.shields.io/badge/Azure-Cloud-0078D4.svg)](https://azure.microsoft.com/)
[![Selenium](https://img.shields.io/badge/Selenium-Web_Scraping-43B02A.svg)](https://www.selenium.dev/)

Pipeline end-to-end de an√°lise de dados de futebol, desde web scraping at√© insights avan√ßados usando **Databricks**, **Azure Blob Storage** e **Delta Live Tables**.

## üìã √çndice

- [Vis√£o Geral](#-vis√£o-geral)
- [Arquitetura](#Ô∏è-arquitetura)
- [Features](#-features)
- [Tecnologias](#-tecnologias)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Instala√ß√£o](#-instala√ß√£o)
- [Pipeline de Dados](#-pipeline-de-dados)
- [Tabelas e Schemas](#-tabelas-e-schemas)
- [Casos de Uso](#-casos-de-uso)
- [Contribuindo](#-contribuindo)
- [Autor](#-autor)

---

## üéØ Vis√£o Geral

Este projeto implementa um **pipeline completo de dados** para an√°lise de estat√≠sticas de futebol, integrando:

1. **Web Scraping** (Selenium) - Coleta de dados do Flashscore
2. **Azure Blob Storage** - Armazenamento direto em containers
3. **Databricks** - Processamento com Delta Live Tables
4. **Arquitetura Medallion** - Bronze ‚Üí Silver ‚Üí Gold

### O que o projeto faz?

- üîç **Extrai** dados de 28+ campeonatos (Brasileir√£o, Champions League, Premier League, etc.)
- ‚òÅÔ∏è **Envia diretamente** para Azure Blob Storage (JSON incremental)
- üßπ **Limpa e padroniza** dados com transforma√ß√µes no Databricks
- üìä **Gera insights** avan√ßados: xG Analysis, Performance Tracking, Head-to-Head
- üìà **Disponibiliza** m√©tricas prontas para BI e Machine Learning

---

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  LOCAL - WEB SCRAPING (Python)                   ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Selenium +  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Python     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Upload    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ ChromeDriver ‚îÇ    ‚îÇ  Processing  ‚îÇ    ‚îÇ   Direto     ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AZURE BLOB STORAGE                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Container   ‚îÇ    ‚îÇ  Container   ‚îÇ    ‚îÇ  Container   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  calendar    ‚îÇ    ‚îÇ   results    ‚îÇ    ‚îÇ  statistics  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (JSON)      ‚îÇ    ‚îÇ   (JSON)     ‚îÇ    ‚îÇ   (JSON)     ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DATABRICKS - DELTA LIVE TABLES                      ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  ü•â BRONZE LAYER (Auto Loader - Cloud Files)          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ bronze.calendar      (from Azure Blob)             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ bronze.results       (from Azure Blob)             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ bronze.statistics    (from Azure Blob)             ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                        ‚îÇ                                         ‚îÇ
‚îÇ                        ‚ñº                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  ü•à SILVER LAYER (Curated & Cleaned)                  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ silver.fact_calendar                               ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ silver.fact_finished_matches                       ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ silver.fact_statistics                             ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                        ‚îÇ                                         ‚îÇ
‚îÇ                        ‚ñº                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  ü•á GOLD LAYER (Analytics & Aggregations)             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ gold.dim_matches_complete                          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ gold.fact_team_performance                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ gold.fact_xg_analysis                              ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ gold.fact_head_to_head                             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ gold.fact_attack_defense_metrics                   ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONSUMPTION LAYER                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Power BI    ‚îÇ    ‚îÇ  Notebooks   ‚îÇ    ‚îÇ   ML Models  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Dashboards  ‚îÇ    ‚îÇ   Analysis   ‚îÇ    ‚îÇ  Predictions ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fluxo de Dados Detalhado

1. **Scraping Local**: Scripts Python com Selenium coletam dados do Flashscore
2. **Upload Direto**: Dados enviados imediatamente para Azure Blob Storage (JSON)
3. **Bronze Layer**: Auto Loader detecta novos arquivos e cria Delta Tables
4. **Silver Layer**: Limpeza, padroniza√ß√£o e enriquecimento (data quality checks)
5. **Gold Layer**: M√©tricas agregadas e analytics-ready (business logic)

> **‚ö†Ô∏è IMPORTANTE**: N√£o h√° camada de armazenamento local/raw. O scraping envia **diretamente** para o Azure!

---

## ‚ú® Features

### Web Scraping
- ‚úÖ **Multi-campeonatos**: 28+ ligas nacionais e internacionais
- ‚úÖ **Coleta Incremental**: Atualiza apenas novos dados (√∫ltimos N dias configur√°vel)
- ‚úÖ **Upload Direto**: Envia JSON diretamente para Azure Blob Storage
- ‚úÖ **Retry Autom√°tico**: Sistema robusto com 3 tentativas por URL
- ‚úÖ **Merge Inteligente**: Evita duplicatas usando IDs √∫nicos
- ‚úÖ **Logs Detalhados**: Monitoramento completo do processo

### Notebooks de Scraping

#### 1. `get_calendar.py` - Calend√°rio de Partidas
- Coleta jogos **futuros agendados**
- Container Azure: `calendar`
- Output: `all_calendar.json`
- Inclui: data, hora, times, campeonato, temporada, rodada

#### 2. `get_results.py` - Resultados de Partidas
- Coleta resultados dos **√∫ltimos N dias** (configur√°vel)
- Container Azure: `results`
- Output: `all_results_incremental.json`
- **Upload incremental**: merge com dados existentes
- Inclui: placares, data, times, rodada

#### 3. `get_statistics_urls.py` - URLs de Estat√≠sticas
- Extrai links de estat√≠sticas dos jogos recentes
- Container Azure: `statistics-urls`
- Output: `statistics_urls.json`
- Filtro: √∫ltimos N dias configur√°vel

#### 4. `get_statistics.py` - Estat√≠sticas Detalhadas
- Baixa URLs do Azure e coleta estat√≠sticas completas
- Container Azure: `statistics`
- Output: `incremental_games_statistics.json`
- **Checkpoint a cada 100 jogos**
- Inclui: xG, posse, chutes, passes, cart√µes, etc.

### Campeonatos Suportados
- üáßüá∑ **Brasil**: S√©rie A, S√©rie B, Copa do Brasil
- üá¶üá∑ **Argentina**: Primera Divisi√≥n
- üèÜ **Am√©rica do Sul**: Libertadores, Sul-Americana
- üè¥ **Inglaterra**: Premier League, Championship
- üá™üá∏ **Espanha**: La Liga
- üáÆüáπ **It√°lia**: Serie A
- üá©üá™ **Alemanha**: Bundesliga
- üá´üá∑ **Fran√ßa**: Ligue 1
- üáµüáπ **Portugal**: Liga Portugal
- üá≥üá± **Holanda**: Eredivisie
- üèÜ **Europa**: Champions League, Europa League
- üá∏üá¶ **Ar√°bia Saudita**: Saudi Pro League
- üá®üá≥ **China**: Super Liga Chinesa
- üáπüá∑ **Turquia**: S√ºper Lig
- E muitos outros...

### Pipeline Databricks
- ‚úÖ **Delta Live Tables**: Pipelines declarativos e auto-gerenciados
- ‚úÖ **Auto Loader (Cloud Files)**: Detecta novos arquivos automaticamente
- ‚úÖ **Streaming**: Processamento incremental
- ‚úÖ **Data Quality**: Valida√ß√µes e constraints em cada camada
- ‚úÖ **Otimiza√ß√£o**: Z-Order e particionamento para performance

### Analytics
- ‚úÖ **xG Analysis**: Compara√ß√£o entre gols esperados (xG) vs reais
- ‚úÖ **Team Performance**: Rankings, pontos, saldo de gols, vit√≥rias
- ‚úÖ **Head-to-Head**: Hist√≥rico completo de confrontos diretos
- ‚úÖ **Attack/Defense Metrics**: Efici√™ncia ofensiva e defensiva
- ‚úÖ **Shot Accuracy**: Precis√£o de finaliza√ß√£o por time
- ‚úÖ **Possession Analysis**: An√°lise de posse de bola

---

## üõ†Ô∏è Tecnologias

### Scraping & Upload
- **Python 3.11+**
- **Selenium** - Web scraping automatizado
- **ChromeDriver** - Driver para automa√ß√£o do Chrome
- **Azure SDK (azure-storage-blob)** - Upload direto para Blob Storage
- **python-dotenv** - Gerenciamento de vari√°veis de ambiente

### Processamento (Databricks)
- **PySpark** - Processamento distribu√≠do
- **Delta Lake** - Storage ACID com versionamento
- **Delta Live Tables (DLT)** - Pipeline orchestration
- **Auto Loader** - Detec√ß√£o autom√°tica de novos arquivos
- **Azure Databricks** - Plataforma lakehouse

### Storage & Infrastructure
- **Azure Blob Storage** - Armazenamento de JSONs
- **Delta Tables** - Tabelas anal√≠ticas ACID
- **Unity Catalog** - Governan√ßa de dados (opcional)

---

## üìÅ Estrutura do Projeto

```
databricks_football_scraping/
‚îÇ
‚îú‚îÄ‚îÄ üìì notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ get_calendar.py                  # ‚úÖ Scraping de calend√°rio ‚Üí Azure
‚îÇ   ‚îú‚îÄ‚îÄ get_results.py                   # ‚úÖ Scraping de resultados ‚Üí Azure
‚îÇ   ‚îú‚îÄ‚îÄ get_statistics_urls.py           # ‚úÖ Coleta URLs de estat√≠sticas ‚Üí Azure
‚îÇ   ‚îî‚îÄ‚îÄ get_statistics.py                # ‚úÖ Scraping de estat√≠sticas ‚Üí Azure
‚îÇ
‚îú‚îÄ‚îÄ üîÑ dlt_pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest_calendar.py           # Auto Loader: Azure ‚Üí Bronze
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest_results.py            # Auto Loader: Azure ‚Üí Bronze
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ingest_statistics.py         # Auto Loader: Azure ‚Üí Bronze
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fact_calendar.py             # Calend√°rio limpo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fact_finished_matches.py     # Resultados limpos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fact_statistics.py           # Estat√≠sticas limpas
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ gold/
‚îÇ       ‚îú‚îÄ‚îÄ dim_matches_complete.py      # Vis√£o consolidada
‚îÇ       ‚îú‚îÄ‚îÄ fact_team_performance.py     # Agrega√ß√£o por time
‚îÇ       ‚îú‚îÄ‚îÄ fact_xg_analysis.py          # An√°lise xG
‚îÇ       ‚îú‚îÄ‚îÄ fact_head_to_head.py         # Confrontos diretos
‚îÇ       ‚îî‚îÄ‚îÄ fact_attack_defense_metrics.py
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è config/
‚îÇ   ‚îî‚îÄ‚îÄ all_url.json                     # Lista de URLs dos campeonatos
‚îÇ
‚îú‚îÄ‚îÄ üìÑ .env                               # Credenciais Azure (gitignored)
‚îú‚îÄ‚îÄ üìÑ requirements.txt                   # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ .gitignore                        
‚îî‚îÄ‚îÄ üìÑ README.md                          # Este arquivo
```

---

## üöÄ Instala√ß√£o

### Pr√©-requisitos

- Python 3.11+
- Google Chrome instalado
- Conta Azure com Blob Storage
- Workspace Databricks (Azure Databricks)
- Git

### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/DiogodsRibeiro/databricks_football_scraping.git
cd databricks_football_scraping
```

### 2. Configure o ambiente Python

```bash
# Crie um ambiente virtual
python -m venv venv

# Ative o ambiente
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Instale as depend√™ncias
pip install -r requirements.txt
```

**requirements.txt:**
```txt
selenium==4.15.0
webdriver-manager==4.0.1
azure-storage-blob==12.19.0
python-dotenv==1.0.0
```

### 3. Configure o ChromeDriver

```bash
# Op√ß√£o 1: Usando webdriver-manager (recomendado)
pip install webdriver-manager

# O c√≥digo j√° usa automaticamente:
from webdriver_manager.chrome import ChromeDriverManager
```

### 4. Configure Azure Blob Storage

Crie um arquivo `.env` na raiz do projeto:

```env
# ‚ö†Ô∏è N√ÉO COMMITAR ESTE ARQUIVO!
AZURE_STORAGE_CONNECTION_STRING="DefaultEndpointsProtocol=https;AccountName=seu_storage;AccountKey=sua_chave;EndpointSuffix=core.windows.net"

# Containers (opcionais, j√° tem defaults no c√≥digo)
AZURE_CONTAINER_NAME="results"
```

**Como obter a connection string:**
1. Acesse o Portal Azure ‚Üí Storage Account
2. V√° em "Access Keys"
3. Copie a "Connection String" da Key 1 ou Key 2

**Containers criados automaticamente:**
- `calendar` - Calend√°rio de jogos futuros
- `results` - Resultados de jogos finalizados
- `statistics-urls` - URLs das estat√≠sticas
- `statistics` - Estat√≠sticas detalhadas das partidas

### 5. Configure Databricks

```bash
# Instale Databricks CLI
pip install databricks-cli

# Configure autentica√ß√£o
databricks configure --token

# Ser√° solicitado:
# Host: https://adb-XXXXXXXXX.XX.azuredatabricks.net
# Token: dapi... (gere em User Settings ‚Üí Access Tokens)
```

### 6. Monte Azure Blob Storage no Databricks

No Databricks, execute em um notebook:

```python
# Montar Azure Blob Storage
storage_account_name = "seu_storage_account"
storage_account_key = "sua_chave"
container_name = "calendar"  # Repita para cada container

dbutils.fs.mount(
  source = f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net",
  mount_point = f"/mnt/{container_name}",
  extra_configs = {
    f"fs.azure.account.key.{storage_account_name}.blob.core.windows.net": storage_account_key
  }
)

# Verificar
dbutils.fs.ls(f"/mnt/{container_name}")
```

---

## üîÑ Pipeline de Dados

### Fase 1: Web Scraping ‚Üí Azure (Local - Python)

#### 1.1 Calend√°rio de Partidas

```python
# notebooks/get_calendar.py

from get_calendar import carga_calendario

# Executa scraping e upload para Azure
carga_calendario()

# Resultado:
# ‚úÖ Arquivo salvo no Azure: all_calendar.json
# Container: calendar
```

**O que faz:**
- Acessa URLs configuradas em `data/json/all_url.json`
- Extrai calend√°rio (jogos futuros)
- Faz upload direto para Azure Blob (container `calendar`)
- Formato: `{"origem": "Brasil", "Campeonato": "Brasileir√£o A", ...}`

#### 1.2 Resultados de Partidas (Incremental)

```python
# notebooks/get_results.py

from get_results import carga_incremental_results

# √öltimos 2 dias (padr√£o)
carga_incremental_results()

# Ou carregar todos os resultados
carga_incremental_results(carregar_todos=True)

# Resultado:
# ‚úÖ Arquivo salvo no Azure: all_results_incremental.json
# Container: results
```

**Configura√ß√£o:**
```python
DIAS_RETROATIVOS = 2  # Ajuste conforme necess√°rio
```

**O que faz:**
- Busca resultados dos √∫ltimos N dias
- **Merge inteligente**: baixa JSON existente do Azure, evita duplicatas
- Upload incremental (n√£o sobrescreve dados antigos)
- Checkpoint autom√°tico

#### 1.3 URLs de Estat√≠sticas

```python
# notebooks/get_statistics_urls.py

from get_statistics_urls import coletar_urls_estatisticas

# Coleta URLs dos √∫ltimos 2 dias
coletar_urls_estatisticas()

# Resultado:
# ‚úÖ Arquivo salvo no Azure: statistics_urls.json
# Container: statistics-urls
```

**O que faz:**
- Acessa p√°ginas de resultados
- Filtra jogos dos √∫ltimos N dias
- Extrai URLs de estat√≠sticas detalhadas
- Salva lista de URLs no Azure

#### 1.4 Estat√≠sticas Detalhadas

```python
# notebooks/get_statistics.py

from get_statistics import coletar_estatisticas_partidas_incremental

# Baixa URLs do Azure e coleta estat√≠sticas
coletar_estatisticas_partidas_incremental()

# Resultado:
# ‚úÖ Arquivo salvo no Azure: incremental_games_statistics.json
# Container: statistics
# üíæ Checkpoint a cada 100 jogos
```

**Features:**
- **Retry autom√°tico**: 3 tentativas por URL
- **Checkpoint**: salva progresso a cada 100 jogos
- **Merge incremental**: evita duplicatas
- **Fallback de seletores**: m√∫ltiplos CSS selectors para robustez
- **Log de falhas**: salva URLs com erro em `urls_com_falha.json`

**Configura√ß√£o:**
```python
DIAS_RETROATIVOS = 2  # Ajuste conforme necess√°rio
```

---

### Fase 2: Azure ‚Üí Databricks (Bronze Layer)

#### Auto Loader - Detec√ß√£o Autom√°tica de Arquivos

```python
# dlt_pipelines/bronze/ingest_calendar.py

import dlt
from pyspark.sql.functions import col, current_timestamp

@dlt.table(
    name="bronze.calendar",
    comment="Raw calendar data from Azure Blob Storage",
    table_properties={
        "quality": "bronze",
        "pipelines.autoOptimize.managed": "true"
    }
)
def bronze_calendar():
    """
    Auto Loader detecta automaticamente novos arquivos em /mnt/calendar
    e carrega incrementalmente para a tabela bronze.calendar
    """
    return (
        spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.schemaLocation", "/mnt/schemas/bronze/calendar")
        .option("cloudFiles.inferColumnTypes", "true")
        .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
        .load("/mnt/calendar/")
        .withColumn("_ingestion_timestamp", current_timestamp())
        .withColumn("_source_file", col("_metadata.file_path"))
    )
```

**Principais features:**
- ‚úÖ **Auto Loader**: detecta novos arquivos automaticamente
- ‚úÖ **Schema Evolution**: adiciona novas colunas automaticamente
- ‚úÖ **Checkpoint**: rastreia arquivos j√° processados
- ‚úÖ **Idempot√™ncia**: n√£o processa o mesmo arquivo 2x

#### Ingest√£o de Resultados

```python
# dlt_pipelines/bronze/ingest_results.py

import dlt
from pyspark.sql.functions import current_timestamp

@dlt.table(
    name="bronze.results",
    comment="Raw match results from Azure Blob Storage"
)
def bronze_results():
    return (
        spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.schemaLocation", "/mnt/schemas/bronze/results")
        .load("/mnt/results/")
        .withColumn("_ingestion_timestamp", current_timestamp())
    )
```

#### Ingest√£o de Estat√≠sticas

```python
# dlt_pipelines/bronze/ingest_statistics.py

import dlt
from pyspark.sql.functions import current_timestamp

@dlt.table(
    name="bronze.statistics",
    comment="Raw match statistics from Azure Blob Storage"
)
def bronze_statistics():
    return (
        spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.schemaLocation", "/mnt/schemas/bronze/statistics")
        .load("/mnt/statistics/")
        .withColumn("_ingestion_timestamp", current_timestamp())
    )
```

---

### Fase 3: Silver Layer - Transforma√ß√£o e Limpeza

#### Calend√°rio Limpo

```python
# dlt_pipelines/silver/fact_calendar.py

import dlt
from pyspark.sql.functions import col, to_timestamp, concat_ws, trim

@dlt.table(
    name="silver.fact_calendar",
    comment="Cleaned and standardized calendar data"
)
@dlt.expect_or_drop("valid_teams", 
    "home_team IS NOT NULL AND away_team IS NOT NULL")
@dlt.expect_or_drop("valid_datetime", 
    "match_datetime IS NOT NULL")
def silver_calendar():
    return (
        dlt.read_stream("bronze.calendar")
        .withColumn("match_datetime", 
            to_timestamp(concat_ws(" ", col("data"), col("hora")), "dd/MM/yyyy HH:mm")
        )
        .select(
            col("id").alias("match_id"),
            col("Campeonato").alias("championship"),
            col("temporada").alias("season"),
            col("rodada").cast("int").alias("round"),
            col("time_casa").alias("home_team"),
            col("time_visitante").alias("away_team"),
            col("origem").alias("country"),
            col("match_datetime")
        )
    )
```

#### Resultados Limpos

```python
# dlt_pipelines/silver/fact_finished_matches.py

import dlt
from pyspark.sql.functions import col

@dlt.table(
    name="silver.fact_finished_matches",
    comment="Cleaned match results"
)
@dlt.expect_or_drop("valid_scores", 
    "placar_casa >= 0 AND placar_visitante >= 0")
def silver_finished_matches():
    return (
        dlt.read_stream("bronze.results")
        .select(
            col("id").alias("match_id"),
            col("campeonato").alias("championship"),
            col("temporada").alias("season"),
            col("rodada").alias("round"),
            col("time_casa").alias("home_team"),
            col("time_visitante").alias("away_team"),
            col("origem").alias("country"),
            col("placar_casa").cast("int").alias("home_score"),
            col("placar_visitante").cast("int").alias("away_score"),
            to_timestamp(concat_ws(" ", col("data"), col("hora")), "dd/MM/yyyy HH:mm").alias("match_date")
        )
    )
```

#### Estat√≠sticas Limpas

```python
# dlt_pipelines/silver/fact_statistics.py

import dlt
from pyspark.sql.functions import col, round as spark_round

@dlt.table(
    name="silver.fact_statistics",
    comment="Cleaned and normalized match statistics"
)
@dlt.expect_or_drop("valid_match_id", "match_id IS NOT NULL")
@dlt.expect("realistic_xg", 
    "expected_goals_xg_home >= 0 AND expected_goals_xg_away >= 0")
@dlt.expect("valid_possession",
    "possession_home + possession_away BETWEEN 95 AND 105")
def silver_statistics():
    return (
        dlt.read_stream("bronze.statistics")
        .select(
            col("id").alias("match_id"),
            col("date").alias("match_date"),
            
            # Expected Goals
            spark_round(col("gols_esperados_xg_home"), 2).alias("expected_goals_xg_home"),
            spark_round(col("gols_esperados_xg_away"), 2).alias("expected_goals_xg_away"),
            
            # Posse
            spark_round(col("posse_de_bola_home"), 1).alias("possession_home"),
            spark_round(col("posse_de_bola_away"), 1).alias("possession_away"),
            
            # Finaliza√ß√µes
            col("total_de_finalizacoes_home").cast("int").alias("total_shots_home"),
            col("total_de_finalizacoes_away").cast("int").alias("total_shots_away"),
            col("finalizacoes_no_alvo_home").cast("int").alias("shots_on_target_home"),
            col("finalizacoes_no_alvo_away").cast("int").alias("shots_on_target_away"),
            
            # Outros...
            col("chances_claras_home").cast("int").alias("big_chances_home"),
            col("chances_claras_away").cast("int").alias("big_chances_away"),
            col("escanteios_home").cast("int").alias("corners_home"),
            col("escanteios_away").cast("int").alias("corners_away")
        )
        .filter(col("match_id").isNotNull())
    )
```

---

### Fase 4: Gold Layer - Analytics

```python
# dlt_pipelines/gold/dim_matches_complete.py

import dlt
from pyspark.sql.functions import col, when, coalesce, round as spark_round

@dlt.table(
    name="gold.dim_matches_complete",
    comment="Complete match view with all metrics for analytics"
)
def gold_matches_complete():
    results = dlt.read("silver.fact_finished_matches")
    stats = dlt.read("silver.fact_statistics")
    
    return (
        results
        .join(stats, results.match_id == stats.match_id, "left")
        .select(
            results.match_id,
            results.match_date.alias("match_datetime"),
            results.championship,
            results.season,
            results.round,
            results.home_team,
            results.away_team,
            results.home_score,
            results.away_score,
            
            # Match Result
            when(results.home_score > results.away_score, "Home Win")
                .when(results.home_score < results.away_score, "Away Win")
                .otherwise("Draw").alias("match_result"),
            
            (results.home_score + results.away_score).alias("total_goals"),
            
            # xG Metrics
            stats.expected_goals_xg_home,
            stats.expected_goals_xg_away,
            spark_round(results.home_score - stats.expected_goals_xg_home, 2).alias("xg_diff_home"),
            spark_round(results.away_score - stats.expected_goals_xg_away, 2).alias("xg_diff_away"),
            
            # Outras estat√≠sticas...
            stats.possession_home,
            stats.possession_away,
            stats.total_shots_home,
            stats.total_shots_away,
            stats.shots_on_target_home,
            stats.shots_on_target_away
        )
    )
```

Para as outras tabelas gold (team_performance, xg_analysis, head_to_head), consulte o c√≥digo completo fornecido anteriormente.

---

## üìä Tabelas e Schemas

### Containers Azure Blob Storage

| Container | Descri√ß√£o | Arquivo | Atualiza√ß√£o |
|-----------|-----------|---------|-------------|
| `calendar` | Calend√°rio futuro | `all_calendar.json` | Full refresh |
| `results` | Resultados hist√≥ricos | `all_results_incremental.json` | Incremental |
| `statistics-urls` | URLs de stats | `statistics_urls.json` | Full refresh |
| `statistics` | Estat√≠sticas detalhadas | `incremental_games_statistics.json` | Incremental |

### Tabelas Bronze (Delta)

| Tabela | Origem | Tipo |
|--------|--------|------|
| `bronze.calendar` | Azure Blob `/mnt/calendar` | Streaming |
| `bronze.results` | Azure Blob `/mnt/results` | Streaming |
| `bronze.statistics` | Azure Blob `/mnt/statistics` | Streaming |

### Tabelas Silver (Delta)

| Tabela | Descri√ß√£o | Key Columns |
|--------|-----------|-------------|
| `silver.fact_calendar` | Calend√°rio limpo | match_id, match_datetime |
| `silver.fact_finished_matches` | Resultados limpos | match_id, home_score, away_score |
| `silver.fact_statistics` | Estat√≠sticas limpas | match_id, xg_home, xg_away |

### Tabelas Gold (Delta)

| Tabela | Descri√ß√£o | Use Case |
|--------|-----------|----------|
| `gold.dim_matches_complete` | Vis√£o completa | An√°lise de jogos |
| `gold.fact_team_performance` | Performance por time | Rankings |
| `gold.fact_xg_analysis` | An√°lise xG | Over/Under performance |
| `gold.fact_head_to_head` | Confrontos diretos | Rivalidades |
| `gold.fact_attack_defense_metrics` | M√©tricas de jogo | Efici√™ncia |

---

## üí° Casos de Uso

### 1. Ranking de Times (Tabela de Classifica√ß√£o)

```sql
SELECT 
    team,
    total_games,
    total_wins,
    total_draws,
    total_losses,
    total_points,
    goal_difference,
    ROUND(total_points / NULLIF(total_games, 0), 2) as points_per_game
FROM gold.fact_team_performance
WHERE championship = 'Brasileir√£o A'
  AND season = '2024/2025'
ORDER BY total_points DESC, goal_difference DESC
LIMIT 20;
```

### 2. Times que Super Performam (xG Analysis)

```sql
WITH team_xg AS (
    SELECT 
        home_team as team,
        AVG(xg_diff_home) as avg_xg_overperformance,
        COUNT(*) as matches
    FROM gold.fact_xg_analysis
    WHERE season = '2024/2025'
    GROUP BY home_team
    
    UNION ALL
    
    SELECT 
        away_team as team,
        AVG(xg_diff_away) as avg_xg_overperformance,
        COUNT(*) as matches
    FROM gold.fact_xg_analysis
    WHERE season = '2024/2025'
    GROUP BY away_team
)

SELECT 
    team,
    ROUND(AVG(avg_xg_overperformance), 2) as xg_overperformance,
    SUM(matches) as total_matches
FROM team_xg
GROUP BY team
HAVING SUM(matches) >= 10
ORDER BY xg_overperformance DESC
LIMIT 10;
```

### 3. An√°lise de Confronto Direto

```sql
SELECT 
    home_team,
    away_team,
    COUNT(*) as total_matches,
    SUM(CASE WHEN match_result = 'Home Win' THEN 1 ELSE 0 END) as home_wins,
    SUM(CASE WHEN match_result = 'Away Win' THEN 1 ELSE 0 END) as away_wins,
    SUM(CASE WHEN match_result = 'Draw' THEN 1 ELSE 0 END) as draws,
    AVG(total_goals) as avg_goals
FROM gold.dim_matches_complete
WHERE (home_team = 'Flamengo' AND away_team = 'Palmeiras')
   OR (home_team = 'Palmeiras' AND away_team = 'Flamengo')
GROUP BY home_team, away_team;
```

### 4. Dashboard Executivo - KPIs

```sql
SELECT 
    COUNT(DISTINCT match_id) as total_matches,
    SUM(total_goals) as total_goals,
    ROUND(AVG(total_goals), 2) as avg_goals_per_match,
    ROUND(AVG(total_xg), 2) as avg_xg_per_match,
    COUNT(DISTINCT home_team) as total_teams
FROM gold.dim_matches_complete
WHERE season = '2024/2025'
  AND championship = 'Brasileir√£o A';
```

---

## üìà Monitoramento

### Data Quality Checks

```python
# Expectations implementadas no DLT

@dlt.expect_or_drop("valid_teams", "home_team IS NOT NULL AND away_team IS NOT NULL")
@dlt.expect_or_drop("valid_score", "home_score >= 0 AND away_score >= 0")
@dlt.expect_or_warn("realistic_possession", "possession_home + possession_away BETWEEN 95 AND 105")
@dlt.expect_or_fail("match_id_unique", "COUNT(DISTINCT match_id) = COUNT(*)")
```

### Logs do Scraping

```python
# Exemplo de sa√≠da do get_statistics.py

üîÑ Processando URL 1/150: https://...
‚úÖ Coletado: Flamengo_vs_Palmeiras_15012024
üíæ Progresso salvo com 100 jogos.
‚ùå Erro ao processar URL (tentativa 1/3): Timeout
üîÑ Reiniciando navegador em 10 segundos...
‚úÖ {len(todos_os_jogos)} jogos coletados com sucesso!
‚ùå {len(urls_com_falha)} URLs falharam
```

### M√©tricas do Pipeline DLT

Acesse: **Databricks UI ‚Üí Delta Live Tables ‚Üí [Pipeline]**
- Tempo de execu√ß√£o por tabela
- Registros processados
- Data quality violations
- Resource utilization

---

## üîß Configura√ß√£o Avan√ßada

### Ajustar Per√≠odo de Coleta

```python
# Em get_results.py, get_statistics_urls.py, get_statistics.py
DIAS_RETROATIVOS = 7  # Altere para coletar mais dias
```

### Adicionar Novos Campeonatos

Edite `data/json/all_url.json`:

```json
{
  "urls": [
    "https://www.flashscore.com.br/futebol/brasil/serie-a/{endpoint}/",
    "https://www.flashscore.com.br/futebol/europa/champions-league/{endpoint}/",
    "SUA_NOVA_URL/{endpoint}/"
  ]
}
```

### Otimiza√ß√£o de Performance

```sql
-- Z-Ordering
OPTIMIZE gold.dim_matches_complete
ZORDER BY (championship, season, match_datetime, home_team);

-- Vacuum (remover arquivos antigos)
VACUUM gold.dim_matches_complete RETAIN 168 HOURS;
```

### Scheduling Autom√°tico

```bash
# Databricks Jobs - Agendar scraping + pipeline
# 1. Crie um Job no Databricks UI
# 2. Adicione tasks:
#    - Task 1: Executar notebooks Python (scraping)
#    - Task 2: Trigger DLT Pipeline
# 3. Schedule: Cron "0 6 * * *" (6h da manh√£ diariamente)
```

---

</div>
